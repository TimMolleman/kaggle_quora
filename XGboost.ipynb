{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmolleman/anaconda/envs/Machine_learning_36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk import ngrams, bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # Read in training data\n",
    "    data_train = pd.read_csv('Data/train_data.csv')\n",
    "    data_train = data_train.drop('is_duplicate', axis=1)\n",
    "    train_labels = pd.read_csv('Data/train_labels.csv')\n",
    "    data_train = pd.merge(data_train, train_labels, on='id')\n",
    "    test_data = pd.read_csv('Data/test_data.csv')\n",
    "    \n",
    "    return [train_labels, data_train, test_data]\n",
    "\n",
    "train_labels, train_df, validation_data = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Question Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did harry become a horcrux?\n",
      "What is a Horcrux?\n"
     ]
    }
   ],
   "source": [
    "q1_example = train_data['question1'].iloc[80]\n",
    "q2_example = train_data['question2'].iloc[80]\n",
    "print(q1_example)\n",
    "print(q2_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanup(question, stop_remove = True, stemming=True):\n",
    "    \n",
    "    # Split question and lower\n",
    "    question = question.lower()\n",
    "\n",
    "    # Regex the question and split \n",
    "    clean_question = clean_text(question).split()\n",
    "    \n",
    "    # Remove stopwords if True\n",
    "    if stop_remove:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        clean_question = [word for word in clean_question if word not in stops]\n",
    "    \n",
    "    # Stem words in the questions if True\n",
    "    if stemming:\n",
    "        stemmer = LancasterStemmer()\n",
    "        clean_question = [stemmer.stem(word) for word in clean_question]\n",
    "    \n",
    "    clean_question = \" \".join(clean_question)\n",
    "    \n",
    "    return clean_question\n",
    "\n",
    "# Cleaning text in training and testing df's\n",
    "train_df['q1_clean'] = train_df['question1'].apply(lambda question: cleanup(str(question)))\n",
    "train_df['q2_clean'] = train_df['question2'].apply(lambda question: cleanup(str(question)))\n",
    "validation_data['q1_clean'] = validation_data['question1'].apply(lambda question: cleanup(str(question)))\n",
    "validation_data['q2_clean'] = validation_data['question2'].apply(lambda question: cleanup(str(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text DF to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to write cleaned DF to CSV? \n",
      "y/n n\n",
      "\n",
      "\n",
      "Do you want to load the cleaned DF from CSV then?\n",
      "y/n n\n"
     ]
    }
   ],
   "source": [
    "print ('Do you want to write cleaned DF to CSV? ')\n",
    "answer = input('y/n ')\n",
    "print (\"\\n\")\n",
    "\n",
    "if answer == 'y':\n",
    "    train_df.to_csv('Data/cleaned_train.csv')\n",
    "    validation_data.to_csv('Data/cleaned_validation.csv')\n",
    "\n",
    "elif answer == 'n':\n",
    "    print ('Do you want to load the cleaned DF from CSV then?')\n",
    "    answer2 = input('y/n ')\n",
    "    \n",
    "    if answer2 == 'y':\n",
    "        train_df = pd.read_csv('Data/cleaned_train.csv')\n",
    "        validation_data = pd.read_csv('Data/cleaned_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Weights for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create the corpus for TF-IDF\n",
    "q1_corpus = [str(question).lower() for question in list(train_df['question1'])]\n",
    "q2_corpus = [str(question).lower() for question in list(train_df['question2'])]\n",
    "corpus = q1_corpus + q2_corpus\n",
    "\n",
    "# Get weights\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000\n",
    "words = (\" \".join(corpus)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Shared TF-IDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmolleman/anaconda/envs/Machine_learning_36/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/timmolleman/anaconda/envs/Machine_learning_36/lib/python3.6/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "# Apply tf-idf function to train and validation sets\n",
    "tdif_train_data = train_df.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "tdif_validation_data = validation_data.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "train_df['tf-idf'] = tdif_train_data\n",
    "validation_data['tf-idf'] = tdif_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get no. character difference between question 1 and question 2\n",
    "def char_count_difference(df):\n",
    "    char_count_dif = abs(df['char_counts_q1'] - df['char_counts_q2'])\n",
    "    return char_count_dif\n",
    "\n",
    "# Get word count difference between question 1 and question 2\n",
    "def word_count_difference(df):\n",
    "    word_count_dif = abs(df['word_counts_q1'] - df['word_counts_q2'])\n",
    "    return word_count_dif\n",
    "\n",
    "# Checks if first word in question is the same for both questions\n",
    "def first_word_same(df):\n",
    "    q1_first_word = df['question1'].apply(lambda x: str(x).split()[0])\n",
    "    q2_first_word = df['question2'].apply(lambda x: str(x).split()[0])\n",
    "    is_same = q1_first_word == q2_first_word\n",
    "    return is_same\n",
    "\n",
    "# Check if the last word in questions\n",
    "def last_word_same(df):\n",
    "    q1_last_word = df['question1'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n",
    "    q2_last_word = df['question2'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n",
    "    is_same = q1_last_word == q2_last_word\n",
    "    return is_same\n",
    "\n",
    "# Check if the first word as well as the last word is the same\n",
    "def first_last_same(df):\n",
    "    q1_first_word = df['question1'].apply(lambda x: str(x).split()[0])\n",
    "    q2_first_word = df['question2'].apply(lambda x: str(x).split()[0])\n",
    "    is_same1 = q1_first_word == q2_first_word\n",
    "    \n",
    "    q1_last_word = df['question1'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n",
    "    q2_last_word = df['question2'].apply(lambda x: str(x).split()[len(str(x).split()) - 1])\n",
    "    is_same2 = q1_last_word == q2_last_word\n",
    "    \n",
    "    both_same = (is_same1.astype(int) + is_same2.astype(int)) == 2\n",
    "    return both_same\n",
    "    \n",
    "# Checks how many word matches there are between the two questions\n",
    "def intersection(df):\n",
    "    words_q1 = df['q1_clean'].apply(lambda x: str(x).split())\n",
    "    words_q2 = df['q2_clean'].apply(lambda x: str(x).split())\n",
    "    intersections = [set(q1).intersection(set(q2)) for q1, q2 in zip(words_q1, words_q2)]\n",
    "    same_count = [len(intersection) for intersection in intersections]\n",
    "    return same_count\n",
    "\n",
    "# Intersections on words in raw sentences\n",
    "def intersection_raw(df):\n",
    "    words_q1 = df['question1'].apply(lambda x: str(x).split())\n",
    "    words_q2 = df['question2'].apply(lambda x: str(x).split())\n",
    "    intersections = [set(q1).intersection(set(q2)) for q1, q2 in zip(words_q1, words_q2)]\n",
    "    same_count = [len(intersection) for intersection in intersections]\n",
    "    return same_count\n",
    "\n",
    "# Intersection ratio with respect to number of words in q1\n",
    "def intersection_q1(df):\n",
    "    words_q1 = df['q1_clean'].apply(lambda x: str(x).split())\n",
    "    words_q2 = df['q2_clean'].apply(lambda x: str(x).split())\n",
    "    words_q1_len = df['q1_clean'].apply(lambda x: len(str(x).split()))\n",
    "    intersections = [set(q1).intersection(set(q2)) for q1, q2 in zip(words_q1, words_q2)]\n",
    "    same_count = [len(intersection) for intersection in intersections]\n",
    "    same_count_ratio = same_count / words_q1_len.astype(float)\n",
    "    return same_count_ratio\n",
    "\n",
    "# Intersection ratio with respect to number of words in q2\n",
    "def intersection_q2(df):\n",
    "    words_q1 = df['q1_clean'].apply(lambda x: str(x).split())\n",
    "    words_q2 = df['q2_clean'].apply(lambda x: str(x).split())\n",
    "    words_q2_len = df['q2_clean'].apply(lambda x: len(str(x).split()))\n",
    "    intersections = [set(q1).intersection(set(q2)) for q1, q2 in zip(words_q1, words_q2)]\n",
    "    same_count = [len(intersection) for intersection in intersections]\n",
    "    same_count_ratio = same_count / words_q2_len.astype(float)\n",
    "    return same_count_ratio\n",
    "\n",
    "# Intersection ratio with respect to average number of words in q1/q2\n",
    "def intersection_mean(df):\n",
    "    words_q1 = df['question1'].apply(lambda x: str(x).split())\n",
    "    words_q2 = df['question2'].apply(lambda x: str(x).split())\n",
    "    words_q1_len = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "    words_q2_len = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "    intersections = [set(q1).intersection(set(q2)) for q1, q2 in zip(words_q1, words_q2)]\n",
    "    same_count = [float((len(intersection) * 2)) / (words_q1_len.iloc[i] + words_q2_len.iloc[i]) for i, \n",
    "                  intersection in enumerate(intersections)]\n",
    "    return same_count\n",
    "\n",
    "# Checks if the sentence is the same on cleaned question\n",
    "def same_sentence(df):\n",
    "    q1_sentence = df['q1_clean'].apply(lambda x: str(x).split())\n",
    "    q2_sentence = df['q2_clean'].apply(lambda x: str(x).split())\n",
    "    is_same = q1_sentence == q2_sentence\n",
    "    return is_same\n",
    "\n",
    "# Checks if the sentence is the same on raw questions\n",
    "def same_sentence_raw(df):\n",
    "    q1_sentence = df['question1'].apply(lambda x: str(x).split())\n",
    "    q2_sentence = df['question2'].apply(lambda x: str(x).split())\n",
    "    is_same = q1_sentence == q2_sentence\n",
    "    return is_same\n",
    "\n",
    "# Checks if both question 1 and question 2 contain full-stops\n",
    "def full_stop_count(df):\n",
    "    q1_stops = list(df['question1'].apply(lambda x: '.' in str(x)))\n",
    "    q2_stops = list(df['question2'].apply(lambda x: '.' in str(x)))\n",
    "    \n",
    "    both_stops = [True if q1_stop == True and q2_stop == True else False for q1_stop, q2_stop in zip(q1_stops, q2_stops)]\n",
    "    return both_stops \n",
    "\n",
    "# Counts number of full stops in question 1\n",
    "def full_stop_q1(question):\n",
    "    q1_stops = [1 if char == '.' else 0 for char in list(str(question))]\n",
    "    return sum(q1_stops)\n",
    "\n",
    "# Counts number of full stops in question 2\n",
    "def full_stop_q2(question):\n",
    "    q2_stops = [1 if char == '.' else 0 for char in list(str(question))]\n",
    "    return sum(q2_stops)\n",
    "\n",
    "# Checks if both question 1 and question 2 contain full-stops\n",
    "def start_capital_letter(df):\n",
    "    q1_caps = df['question1'].apply(lambda x: str(x)[0].isupper())\n",
    "    q2_caps = df['question2'].apply(lambda x: str(x)[0].isupper())\n",
    "    \n",
    "    both_caps = [True if q1_cap == True and q2_cap == True else False for q1_cap, q2_cap in zip(q1_caps, q2_caps)]\n",
    "    return both_caps\n",
    "\n",
    "# Checks if question 1 starts with capital letter\n",
    "def start_cap_q1(df):\n",
    "    q1_caps = list(df['question1'].apply(lambda x: str(x)[0].isupper()))\n",
    "    return q1_caps\n",
    "\n",
    "# Checks if question 2 starts with a capital letter\n",
    "def start_cap_q2(df):\n",
    "    q2_caps = list(df['question2'].apply(lambda x: str(x)[0].isupper()))\n",
    "    return q2_caps\n",
    "\n",
    "# Gets a word frequency for a chosen word in question. Used for words like 'what', 'how', etcetera\n",
    "def get_word_freq(question, target_word):\n",
    "    word_freq = [1 if word == target_word else 0 for word in str(question).lower().split()]\n",
    "    return sum(word_freq)\n",
    "\n",
    "# Get amount of shared 2-length ngrams in questions\n",
    "def get_n2_grams(questions):\n",
    "    q1_grams = list(bigrams(str(questions['question1']).lower().split()))\n",
    "    q2_grams = list(bigrams(str(questions['question2']).lower().split()))\n",
    "    n2_shared = len(set(q1_grams).intersection(set(q2_grams)))\n",
    "    return n2_shared\n",
    "\n",
    "# Get amount of shared 3-length ngrams in questions\n",
    "def get_n3_grams(questions):\n",
    "    q1_grams = list(ngrams(str(questions['question1']).lower().split(), 3))\n",
    "    q2_grams = list(ngrams(str(questions['question2']).lower().split(), 3))\n",
    "    n3_shared = len(set(q1_grams).intersection(set(q2_grams)))\n",
    "    return n3_shared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features for Training/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmolleman/anaconda/envs/Machine_learning_36/lib/python3.6/site-packages/ipykernel/__main__.py:149: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/Users/timmolleman/anaconda/envs/Machine_learning_36/lib/python3.6/site-packages/ipykernel/__main__.py:150: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "def create_features(df):\n",
    "    \n",
    "    # Duplicated q's?\n",
    "    df['duplicated_q1'] = df['question1'].duplicated()\n",
    "    df['duplicated_q2'] = df['question2'].duplicated()\n",
    "    df['both_duplicated'] = np.where((df['duplicated_q1'].astype(bool) == True) & (df['duplicated_q2'].astype(bool) == True)\n",
    "                     , True, False)\n",
    "    \n",
    "    # Character counts and wordcounts\n",
    "    df['char_counts_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['char_counts_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "    df['word_counts_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "    df['word_counts_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Get number of how's, why's, where's etcetera\n",
    "    df['q1_what'] = df['question1'].apply(lambda x: get_word_freq(x, 'what'))\n",
    "    df['q2_what'] = df['question2'].apply(lambda x: get_word_freq(x, 'what'))\n",
    "    df['both_what'] = np.where((df['q1_what'].astype(int) >= 1) & (df['q1_what'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_how'] = df['question1'].apply(lambda x: get_word_freq(x, 'how'))\n",
    "    df['q2_how'] = df['question2'].apply(lambda x: get_word_freq(x, 'how'))\n",
    "    df['both_how'] = np.where((df['q1_how'].astype(int) >= 1) & (df['q2_how'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_where'] = df['question1'].apply(lambda x: get_word_freq(x, 'where'))\n",
    "    df['q2_where'] = df['question2'].apply(lambda x: get_word_freq(x, 'where'))\n",
    "    df['both_where'] = np.where((df['q1_where'].astype(int) >= 1) & (df['q2_where'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_why'] = df['question1'].apply(lambda x: get_word_freq(x, 'why'))\n",
    "    df['q2_why'] = df['question2'].apply(lambda x: get_word_freq(x, 'why'))\n",
    "    df['both_why'] = np.where((df['q1_why'].astype(int) >= 1) & (df['q2_why'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_when'] = df['question1'].apply(lambda x: get_word_freq(x, 'when'))\n",
    "    df['q2_when'] = df['question2'].apply(lambda x: get_word_freq(x, 'when'))\n",
    "    df['both_when'] = np.where((df['q1_when'].astype(int) >= 1) & (df['q2_when'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_who'] = df['question1'].apply(lambda x: get_word_freq(x, 'who'))\n",
    "    df['q2_who'] = df['question2'].apply(lambda x: get_word_freq(x, 'who'))\n",
    "    df['both_who'] = np.where((df['q1_who'].astype(int) >= 1) & (df['q2_who'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_which'] = df['question1'].apply(lambda x: get_word_freq(x, 'which'))\n",
    "    df['q2_which'] = df['question2'].apply(lambda x: get_word_freq(x, 'which'))\n",
    "    df['both_which'] = np.where((df['q1_which'].astype(int) >= 1) & (df['q2_which'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    df['q1_i'] = df['question1'].apply(lambda x: get_word_freq(x, 'i'))\n",
    "    df['q2_i'] = df['question2'].apply(lambda x: get_word_freq(x, 'i'))\n",
    "    df['both_i'] = np.where((df['q1_i'].astype(int) >= 1) & (df['q2_i'].astype(int) >= 1)\n",
    "                     , True, False)\n",
    "    \n",
    "    # Average amount of characters per word\n",
    "    df['avg_word_length_q1'] = df['char_counts_q1'] / df['word_counts_q1']\n",
    "    df['avg_word_length_q2'] = df['char_counts_q2'] / df['word_counts_q2']\n",
    "    df['avg_word_diff'] = abs(df['avg_word_length_q1'] - df['avg_word_length_q2'])\n",
    "    \n",
    "    # Character count difference and word count difference\n",
    "    df['char_count_dif'] = char_count_difference(df)\n",
    "    df['word_count_dif'] = word_count_difference(df)\n",
    "    \n",
    "    # Average word ratio\n",
    "    df['char_word_ratio'] = (df['char_counts_q1'] + df['char_counts_q2']) / (df['word_counts_q1'] +df['word_counts_q2'])\n",
    "    \n",
    "    # Same first and last words\n",
    "    df['first_word_same'] = first_word_same(df)\n",
    "    df['last_word_same'] = last_word_same(df)\n",
    "    df['first_last_same'] = first_last_same(df)\n",
    "    \n",
    "    # Same sentence\n",
    "    df['same_sentence'] = same_sentence(df)\n",
    "    df['same_sentence_raw'] = same_sentence_raw(df)\n",
    "    \n",
    "    # Intersections\n",
    "    df['intersection_raw'] = intersection_raw(df)\n",
    "    df['intersection'] = intersection(df)\n",
    "    df['intersection_q1'] = intersection_q1(df)\n",
    "    df['intersection_q2'] = intersection_q2(df)\n",
    "    df['intersection_mean'] = intersection_mean(df)\n",
    "    \n",
    "    # Full stops\n",
    "    df['full_stop_count'] = full_stop_count(df)\n",
    "    df['q1_stops'] = df['question1'].apply(lambda x: full_stop_q1(x))\n",
    "    df['q2_stops'] = df['question2'].apply(lambda x: full_stop_q2(x))\n",
    "    \n",
    "    # Starting capital letters\n",
    "    df['both_start_capital'] = start_capital_letter(df)\n",
    "    df['q1_caps'] = start_cap_q1(df)\n",
    "    df['q2_caps'] = start_cap_q2(df)\n",
    "    \n",
    "    # Ngrams\n",
    "    df['grams2_shared'] = df.apply(get_n2_grams, axis=1)\n",
    "    df['grams3_shared'] = df.apply(get_n3_grams, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = create_features(train_df)\n",
    "validation_data = create_features(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split of train/validation data\n",
    "model_train, model_test = train_test_split(train_df, test_size=0.2)\n",
    "train_data = model_train.dropna()\n",
    "test_data = model_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put independent variables here\n",
    "independents = ['char_counts_q1', 'char_counts_q2', 'word_counts_q1', 'word_counts_q2', 'avg_word_length_q1',\n",
    "                'avg_word_length_q2', 'avg_word_diff', 'char_count_dif', 'word_count_dif', 'first_word_same', \n",
    "                'last_word_same', 'first_last_same','same_sentence', 'same_sentence_raw', 'intersection',\n",
    "                'intersection_raw','intersection_q1', 'intersection_q2', 'intersection_mean', 'tf-idf',\n",
    "               'char_word_ratio', 'full_stop_count', 'q1_stops', 'q2_stops', 'both_start_capital', 'q1_caps',\n",
    "               'q2_caps', 'q1_what', 'q2_what', 'both_what', 'q1_how', 'q2_how', 'both_how', 'q1_where',\n",
    "               'q2_where', 'both_where', 'q1_why', 'q2_why', 'both_why', 'q1_when', 'q2_when', 'both_when',\n",
    "               'q1_who', 'q2_who', 'both_who', 'q1_which', 'q2_which', 'both_which', 'grams2_shared', 'grams3_shared']\n",
    "\n",
    "X_train, y_train = train_data[independents].as_matrix(), train_data['is_duplicate'].as_matrix()\n",
    "X_test, y_test = test_data[independents].as_matrix(), test_data['is_duplicate'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(X_test, y_test)]\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 8\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train, feature_names=independents)\n",
    "d_valid = xgb.DMatrix(X_test, label=y_test, feature_names=independents)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1500, watchlist, early_stopping_rounds=30, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Test Labels with XGB-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation= validation_data[independents].as_matrix()\n",
    "d_test = xgb.DMatrix(x_validation, feature_names=independents)\n",
    "p_test = bst.predict(d_test, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = validation_data['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub['is_duplicate'] = sub['is_duplicate'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "sub.to_csv('Data/xgbBoost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get F-scores of features and put relative importance in df\n",
    "importance = bst.get_fscore()\n",
    "importance = sorted(importance.items())\n",
    "importance_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "importance_df['fscore'] = importance_df['fscore'] / importance_df['fscore'].sum()\n",
    "importance_df = importance_df.sort_values('fscore', ascending=True)[-10:]\n",
    "\n",
    "# Plot top 10 most important features\n",
    "plt.figure()\n",
    "importance_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('Relative Importance of Features XGBoost')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Statistics and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data and Test Data Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of question pairs\n",
    "question_pairs_train = len(train_df)\n",
    "print(question_pairs_train, ': Length of train-data')\n",
    "question_pairs_test = len(test_data)\n",
    "print(question_pairs_test, ': Length of test-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates/Non-Duplicates Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"is_duplicate\", data=train_labels)\n",
    "plt.title('Number of Duplicates vs Non-Duplicates')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_36]",
   "language": "python",
   "name": "conda-env-Machine_learning_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
