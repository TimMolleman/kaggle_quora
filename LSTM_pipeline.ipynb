{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tim Molleman & David Ruhe: Quora Question Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "import urllib\n",
    "import re\n",
    "import csv\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU?\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors.\n",
    "word_vecs = gensim.models.KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature extraction.\n",
    "\n",
    "def get_extra_features(S1, S2):\n",
    "    word_count_s1 = []\n",
    "    word_count_s2 = []\n",
    "    word_count_diff = []\n",
    "    char_count_s1 = []\n",
    "    char_count_s2 = []\n",
    "    char_count_diff = []\n",
    "    avg_word1 = []\n",
    "    avg_word2 = []\n",
    "    avg_word_diff = []\n",
    "    first_same = []\n",
    "    last_same = []\n",
    "    both_same = []\n",
    "    sentence_same = []\n",
    "    intersections = []\n",
    "    intersections_ratio = []\n",
    "    \n",
    "    for s1, s2 in zip(S1, S2):\n",
    "        words_s1 = len(str(s1).split())\n",
    "        words_s2 = len(str(s2).split())\n",
    "        wordsdiff = words_s1 - words_s2\n",
    "        chars_s1 = len(str(s1).replace(' ', ''))\n",
    "        chars_s2 = len(str(s2).replace(' ', ''))\n",
    "        charsdiff = chars_s1 - chars_s2\n",
    "        avg_word_length1 = chars_s1 / words_s1 if words_s1 > 0 else 0\n",
    "        avg_word_length2 = chars_s2 / words_s2 if words_s2 > 0 else 0\n",
    "        avg_word_length_diff = avg_word_length1 - avg_word_length2\n",
    "        wfirst_same = 0\n",
    "        wlast_same = 0\n",
    "        wboth_same = 0\n",
    "        \n",
    "        if words_s1 > 0 and words_s2 > 0:\n",
    "            wfirst_same = 1 if s1.split()[0] == s2.split()[0] else 0\n",
    "            wlast_same = 1 if s1.split()[-1] == s2.split()[-1] else 0\n",
    "            wboth_same = 1 if first_same == 1 and last_same == 1 else 0\n",
    "        \n",
    "        s_same = 1 if s1 == s2 else 0\n",
    "        \n",
    "        s1s2ints = len(set(s1.split()).intersection(set(s2.split())))\n",
    "        \n",
    "        s1s2ratio = ((s1s2ints * 2) / (words_s1 + words_s2)) if words_s1 > 0 or words_s2 > 0 else 0\n",
    "        \n",
    "        word_count_s1.append(words_s1)\n",
    "        word_count_s2.append(words_s2)\n",
    "        word_count_diff.append(wordsdiff)\n",
    "        char_count_s1.append(chars_s1)\n",
    "        char_count_s2.append(chars_s2)\n",
    "        char_count_diff.append(charsdiff)\n",
    "        avg_word1.append(avg_word_length1)\n",
    "        avg_word2.append(avg_word_length2)\n",
    "        avg_word_diff.append(avg_word_length_diff)\n",
    "        first_same.append(wfirst_same)\n",
    "        last_same.append(wlast_same)\n",
    "        both_same.append(wboth_same)\n",
    "        sentence_same.append(s_same)\n",
    "        intersections.append(s1s2ints)\n",
    "        intersections_ratio.append(s1s2ratio)\n",
    "\n",
    "    \n",
    "    # Minmax scaling\n",
    "    word_count_s1 = minmax_scale(word_count_s1)\n",
    "    word_count_s2 = minmax_scale(word_count_s2)\n",
    "    word_count_diff = minmax_scale(word_count_diff)\n",
    "    char_count_s1 = minmax_scale(char_count_s1)\n",
    "    char_count_s2 = minmax_scale(char_count_s2)\n",
    "    char_count_diff = minmax_scale(char_count_diff)\n",
    "    intersections = minmax_scale(intersections)\n",
    "    intersections_ratio = minmax_scale(intersections_ratio)\n",
    "    \n",
    "    all_feats = [[word_count_s1[i], word_count_s2[i], word_count_diff[i], char_count_s1[i],\n",
    "           char_count_s2[i], char_count_diff[i], avg_word1[i], avg_word2[i], avg_word_diff[i],\n",
    "           first_same[i], last_same[i], both_same[i], sentence_same[i], intersections[i],\n",
    "           intersections_ratio[i]] for i in range(len(S1))]\n",
    "    \n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/train_official.csv') as trf: # This is the train file from https://www.kaggle.com/c/quora-question-pairs\n",
    "    next(trf) # skip header.\n",
    "    rows = csv.reader(trf)\n",
    "    train_data = list(zip(*rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/test_data.csv') as tf:\n",
    "    next(tf) # skip header.\n",
    "    rows = csv.reader(tf)\n",
    "    test_data = list(zip(*rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data[1]))\n",
    "print(len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into variables.\n",
    "train_s1 = train_data[3]\n",
    "train_s2 = train_data[4]\n",
    "training_labels = list(map(int, train_data[5])) # Convert to ints.\n",
    "\n",
    "test_s1 = test_data[1]\n",
    "test_s2 = test_data[2]\n",
    "test_ids = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function. Borrowed from: https://www.kaggle.com/currie32/the-importance-of-cleaning-text/notebook\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "train_s1 = [clean_text(s1) for s1 in train_s1]\n",
    "train_s2 = [clean_text(s2) for s2 in train_s2]\n",
    "test_s1 = [clean_text(s1) for s1 in test_s1]\n",
    "test_s2 = [clean_text(s2) for s2 in test_s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish word_to_ix dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "EMBEDDING_DIM = word_vecs['the'].shape[0]\n",
    "all_words = [word for sentence in (train_s1 + train_s2 + test_s1 + test_s2) for word in sentence.split()]\n",
    "counts = Counter(all_words)\n",
    "corpus = [word[0] for word in counts.most_common()]\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 47500\n",
    "\n",
    "print(len(corpus))\n",
    "\n",
    "assert VOCAB_SIZE <= len(corpus)\n",
    "\n",
    "def get_vocabulary_and_embeddings(corpus):\n",
    "    print (\"Loading vocabulary\")\n",
    "    vocabulary = {'<PAD>': 0, '<EOS>': 1, '<UNKNOWN>': 2}\n",
    "    emb_matrix = np.zeros([VOCAB_SIZE, EMBEDDING_DIM])\n",
    "    \n",
    "    n = 3\n",
    "    for i, word in enumerate(corpus):\n",
    "        if word in word_vecs:\n",
    "            if word not in vocabulary.keys():\n",
    "                vocabulary[word] = n\n",
    "                emb_matrix[n] = word_vecs[word]\n",
    "                n += 1\n",
    "\n",
    "#                 if n%100 == 0:\n",
    "#                     print (\"Words: %s\" % n)\n",
    "\n",
    "                if n == VOCAB_SIZE:\n",
    "                    print (\"Loaded vocabulary and embeddings.\")\n",
    "\n",
    "                    \n",
    "                    return vocabulary, emb_matrix\n",
    "            \n",
    "word_to_ix, emb_matrix = get_vocabulary_and_embeddings(corpus)\n",
    "ix_to_word = {ix:word for word, ix in word_to_ix.items()}\n",
    "\n",
    "assert len(word_to_ix) == emb_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features = get_extra_features(train_s1, train_s2)\n",
    "extra_features_test = get_extra_features(test_s1, test_s2)\n",
    "\n",
    "assert len(test_s1) == len(extra_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Shuffle data.\n",
    "from random import shuffle\n",
    "VALID_SIZE = 20000\n",
    "\n",
    "ixs = list(range(len(training_labels)))\n",
    "\n",
    "shuffle(ixs)\n",
    "\n",
    "train_s1_shuf = []\n",
    "train_s2_shuf = []\n",
    "train_lab_shuf = []\n",
    "train_features_shuf = []\n",
    "\n",
    "valid_s1 = []\n",
    "valid_s2 = []\n",
    "valid_lab = []\n",
    "valid_features = []\n",
    "\n",
    "for i in ixs:\n",
    "    if len(valid_s1) < VALID_SIZE:\n",
    "        valid_s1.append(train_s1[i])\n",
    "        valid_s2.append(train_s2[i])\n",
    "        valid_lab.append(training_labels[i])\n",
    "        valid_features.append(extra_features[i])\n",
    "\n",
    "    \n",
    "    else:\n",
    "        train_s1_shuf.append(train_s1[i])\n",
    "        train_s2_shuf.append(train_s2[i])\n",
    "        train_lab_shuf.append(training_labels[i])\n",
    "        train_features_shuf.append(extra_features[i])\n",
    "\n",
    "\n",
    "train_s1 = train_s1_shuf\n",
    "train_s2 = train_s2_shuf\n",
    "training_labels = train_lab_shuf\n",
    "\n",
    "print(len(training_labels))\n",
    "print(len(valid_lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padder.\n",
    "def pad_seq(seq, max_length):\n",
    "    \n",
    "    assert len(seq) <= 40\n",
    "    seq += [0 for i in range(max_length - len(seq))]\n",
    "    return seq  \n",
    "\n",
    "# Convert sentence to ix.\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \n",
    "    idxs = [to_ix[w] if w in to_ix.keys() else to_ix['<UNKNOWN>'] for w in seq.split()]\n",
    "    idxs = idxs[:39] + [to_ix['<EOS>']]\n",
    "    \n",
    "    assert len(idxs) <= 40\n",
    "        \n",
    "    return idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator.\n",
    "def generator(batch_size, s1, s2, labels, features, test_mode=False):\n",
    "    \n",
    "    if test_mode == False:\n",
    "        assert len(s1) == len(s2) == len(labels)\n",
    "    \n",
    "    size = len(s1)\n",
    "    n_batches = int(np.ceil(size / batch_size))\n",
    "    batch_idx = 0\n",
    "    \n",
    "    for _ in range(n_batches):\n",
    "        \n",
    "        input_s1 = [prepare_sequence(sentence, word_to_ix) for sentence in s1[batch_idx:(batch_idx+batch_size)]]\n",
    "        input_s2 = [prepare_sequence(sentence, word_to_ix) for sentence in s2[batch_idx:(batch_idx+batch_size)]]\n",
    "                \n",
    "        input_s1_pad = [pad_seq(s, 40) for s in input_s1]\n",
    "        input_s2_pad = [pad_seq(s, 40) for s in input_s2]\n",
    "        \n",
    "        extra_features = features[batch_idx:(batch_idx + batch_size)]\n",
    "        extra_features = torch.FloatTensor(extra_features)\n",
    "        \n",
    "        input_s1 = torch.LongTensor(input_s1_pad).view(len(input_s1), -1)  \n",
    "        \n",
    "        input_s2 = torch.LongTensor(input_s2_pad).view(len(input_s2), -1)  \n",
    "        \n",
    "        if test_mode == False:\n",
    "            targets = labels[batch_idx:(batch_idx+batch_size)]\n",
    "            targets = torch.FloatTensor(targets).view(len(targets), 1) # 1 = tagset size.\n",
    "        \n",
    "        if test_mode == False:\n",
    "            assert len(targets) == len(input_s1) == len(input_s2)\n",
    "        \n",
    "        batch_idx += batch_size\n",
    "\n",
    "        if test_mode == False:\n",
    "            yield Variable(input_s1), Variable(input_s2), Variable(targets), Variable(extra_features)\n",
    "        else:\n",
    "            yield Variable(input_s1), Variable(input_s2), Variable(extra_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_s1, input_s2, targets, extra_feats in generator(2, train_s1, train_s2, training_labels, extra_features):\n",
    "    print(train_s1[:2])\n",
    "    print(train_s2[:2])\n",
    "    print(input_s1)\n",
    "    print(input_s2)\n",
    "    print(targets)\n",
    "    print(extra_feats)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "DENSE_DIM = 150\n",
    "FEATS_DIM = 64\n",
    "BATCH_SIZE = 256\n",
    "TAGSET_SIZE = 1\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "LEARNING_RATE = 1e-2\n",
    "N_EPOCH = 45\n",
    "DROP_RATE = 0.25\n",
    "EXTRA_FEATURES = len(extra_features[0])\n",
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate adjuster. This turned out not to be much of a help, and oftentimes we wouldn't use it.\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = LEARNING_RATE * (0.9 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooijmans(2017) hidden state batchnormalizer.\n",
    "from bnlstm import SeparatedBatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIAMLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, dense_dim, feats_size, tagset_size, batch_size, drop_rate, num_feats):\n",
    "        super(SIAMLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dense_dim = dense_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout = drop_rate)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.linmap = nn.Linear(2 * hidden_dim * MAX_LEN, dense_dim)\n",
    "        self.linfeats = nn.Linear(num_feats, feats_size)\n",
    "        \n",
    "        self.fc = nn.Linear(dense_dim + feats_size, tagset_size) \n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(2*MAX_LEN)\n",
    "        self.bn2 = nn.BatchNorm1d(dense_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(dense_dim + feats_size)\n",
    "        self.hidbn = SeparatedBatchNorm1d(hidden_dim, MAX_LEN)\n",
    "        \n",
    "        self.hidden_s1 = self.init_hidden()\n",
    "        self.hidden_s2 = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        c0 = autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        h0 = autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "        if cuda:\n",
    "            c0 = c0.cuda()\n",
    "            h0 = h0.cuda()\n",
    "        \n",
    "        return (c0, h0)\n",
    "        \n",
    "    def forward(self, input_s1, input_s2, extra_feats):\n",
    "                \n",
    "        embedding_s1 = self.embedding(input_s1)\n",
    "        embedding_s2 = self.embedding(input_s2)\n",
    "        \n",
    "        s1 = embedding_s1.permute(1,0,2)\n",
    "        s2 = embedding_s2.permute(1,0,2)\n",
    "        \n",
    "        # Forward propagate RNN       \n",
    "        out1, hidden_s1 = self.lstm(s1, self.hidden_s1)\n",
    "        out2, hidden_s2 = self.lstm(s2, self.hidden_s2)\n",
    "\n",
    "        self.hidden_s1 = (self.hidbn(hidden_layer) for hidden_layer in hidden_s1)\n",
    "        self.hidden_s2 = (self.hidbn(hidden_layer) for hidden_layer in hidden_s2)\n",
    "        \n",
    "        out1 = out1.permute(1,0,2).contiguous()\n",
    "        out2 = out2.permute(1,0,2).contiguous()\n",
    "        \n",
    "        out = torch.cat((out1,out2), dim=1)\n",
    "        \n",
    "        out = self.drop(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        out = F.relu(self.linmap(out.view(len(out), -1)))\n",
    "        \n",
    "        out = self.drop(out)\n",
    "        out = self.bn2(out)\n",
    "                \n",
    "        out_feats = self.linfeats(extra_feats)\n",
    "        \n",
    "        out = torch.cat((out, out_feats), dim=1)\n",
    "        \n",
    "        out = self.drop(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "                \n",
    "        return out\n",
    "\n",
    "model = SIAMLSTM(EMBEDDING_DIM, VOCAB_SIZE, HIDDEN_DIM, DENSE_DIM, FEATS_DIM, TAGSET_SIZE, BATCH_SIZE, DROP_RATE, EXTRA_FEATURES)\n",
    "\n",
    "# Embeddings can't be trained.\n",
    "pre_trained_embs = torch.from_numpy(emb_matrix).float()\n",
    "model.embedding.weight.data = pre_trained_embs\n",
    "model.embedding.weight.requires_grad = False\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "lowest_loss = 200\n",
    "best_score = 0.6\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    \n",
    "    print(\"Starting epoch %s\" % epoch)\n",
    "            \n",
    "    optimizer = adjust_learning_rate(optimizer, epoch) # Optional.\n",
    "    \n",
    "    train_batch_loss = []\n",
    "    train_batch_acc = []\n",
    "    \n",
    "    for input_s1, input_s2, labels, extra_feats in generator(BATCH_SIZE, train_s1, train_s2, training_labels, train_features_shuf):\n",
    "        \n",
    "        if cuda:\n",
    "            input_s1, input_s2, labels, extra_feats = input_s1.cuda(), input_s2.cuda(), labels.cuda(), extra_feats.cuda()\n",
    "            \n",
    "        assert len(input_s1) == len(input_s2) == len(labels)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        model.batch_size = len(labels)\n",
    "        \n",
    "        model.hidden_s1 = model.init_hidden()\n",
    "        model.hidden_s2 = model.init_hidden()   \n",
    "        \n",
    "        outputs = model(input_s1, input_s2, extra_feats)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = F.sigmoid(outputs)\n",
    "        predicted[predicted <= 0.5] = 0\n",
    "        predicted[predicted > 0.5] = 1\n",
    "        \n",
    "        correct = predicted == labels\n",
    "        \n",
    "        batch_acc = len(correct[correct == 1]) / len(labels)\n",
    "                \n",
    "        train_batch_acc.append(batch_acc)\n",
    "        train_batch_loss.append(loss.data[0])\n",
    "    \n",
    "    train_loss.append(np.mean(train_batch_loss))\n",
    "    train_acc.append(np.mean(train_batch_acc))\n",
    "    \n",
    "    val_batch_loss = []\n",
    "    val_batch_acc = []\n",
    "    \n",
    "    # Validation Epoch.\n",
    "    \n",
    "    for val_s1, val_s2, val_lab, extra_feats_val in generator(BATCH_SIZE, valid_s1, valid_s2, valid_lab, valid_features):\n",
    "        \n",
    "        if cuda:\n",
    "            val_s1, val_s2, val_lab, extra_feats_val = val_s1.cuda(), val_s2.cuda(), val_lab.cuda(), extra_feats_val.cuda()\n",
    "\n",
    "        assert len(val_s1) == len(val_s2) == len(val_lab)\n",
    "        \n",
    "        model.batch_size = len(val_lab)\n",
    "        model.hidden_s1 = model.init_hidden()\n",
    "        model.hidden_s2 = model.init_hidden()   \n",
    "\n",
    "        val_outputs = model(val_s1, val_s2, extra_feats_val)\n",
    "        \n",
    "        batch_val_loss = criterion(val_outputs, val_lab)\n",
    "        \n",
    "        predicted = F.sigmoid(val_outputs)\n",
    "        \n",
    "        predicted[predicted <= 0.5] = 0\n",
    "        predicted[predicted > 0.5] = 1\n",
    "        \n",
    "        correct = predicted == val_lab\n",
    "        \n",
    "        batch_val_acc = len(correct[correct == 1]) / len(val_lab)\n",
    "                        \n",
    "        val_batch_acc.append(batch_val_acc)\n",
    "        val_batch_loss.append(batch_val_loss.data[0])    \n",
    "    \n",
    "    \n",
    "    val_loss.append(np.mean(val_batch_loss))\n",
    "    val_acc.append(np.mean(val_batch_acc))\n",
    "    \n",
    "    if np.mean(val_batch_acc) > best_score:\n",
    "        print(\"New best score! %s\" % np.mean(val_batch_acc))\n",
    "        torch.save(model.state_dict(), 'submission_lowloss.pt')\n",
    "        best_score = np.mean(val_batch_acc)\n",
    "    \n",
    "    if np.mean(val_batch_loss) < lowest_loss:\n",
    "        print(\"New lowest loss! %s\" % np.mean(val_batch_loss))\n",
    "        torch.save(model.state_dict(), 'submission_highacc.pt')\n",
    "        lowest_loss = np.mean(val_batch_loss)\n",
    "    \n",
    "    print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
    "        % (epoch, N_EPOCH - 1, train_loss[epoch], val_loss[epoch], train_acc[epoch], val_acc[epoch]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(model, path, test_s1, test_s2, extra_features_test, test_ids, filename):\n",
    "    \n",
    "    predictions = []\n",
    "    submission = pd.DataFrame()\n",
    "    if cuda: model.load_state_dict(torch.load(path))\n",
    "    else : model.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage))\n",
    "    model.eval()\n",
    "       \n",
    "    for t1, t2, extra_t in generator(BATCH_SIZE, test_s1, test_s2, None, extra_features_test, test_mode=True):\n",
    "        \n",
    "        if cuda:\n",
    "            t1, t2, extra_t = t1.cuda(), t2.cuda(), extra_t.cuda()\n",
    "        \n",
    "        model.batch_size = len(t1)\n",
    "        model.hidden_s1 = model.init_hidden()\n",
    "        model.hidden_s2 = model.init_hidden()   \n",
    "\n",
    "        val_outputs = model(t1, t2, extra_t)\n",
    "                \n",
    "        predicted = F.sigmoid(val_outputs)\n",
    "        \n",
    "        predicted[predicted <= 0.5] = 0\n",
    "        predicted[predicted > 0.5] = 1\n",
    "                \n",
    "        predictions += list(map(int, predicted.data.cpu().numpy()))\n",
    "        \n",
    "        \n",
    "    assert len(predictions) == len(test_ids)\n",
    "    \n",
    "    submission['test_id'] = test_ids\n",
    "    submission['is_duplicate'] = predictions\n",
    "    filename = filename + '.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(model, 'submission_lowloss.pt', test_s1, test_s2, extra_features_test, test_ids, 'submission_lowloss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_36]",
   "language": "python",
   "name": "conda-env-Machine_learning_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
